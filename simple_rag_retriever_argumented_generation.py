# -*- coding: utf-8 -*-
"""Simple RAG - Retriever Argumented Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uZZc6EPU4uwZff6hcqOZLauZs407zoP3
"""



"""# Simple RAG using Open AI tools.

### Ask user to enter the key (this keeps it hidden)
"""

from getpass import getpass
os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")

"""### Importing Libaraies"""

import os
import pandas as pd
import numpy as np
from openai import OpenAI
client = OpenAI()

"""### Uploading the file from Local"""

from google.colab import files
uploaded = files.upload()

"""### Simple Pass Through prompt"""

Question  = "How is esethu related to phillemon"

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{
        "role": "user",
        "content":Question
    }]
)

response.choices[0].message.content

"""## Now adding some more instruction to the promt"""

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "system", "content":"Answer in 3 lines as if you a poet"},
        {"role": "user", "content":Question}
              ]
)

print(response.choices[0].message.content)

"""## Now adding RAG"""

df_contents = pd.read_csv("contents .csv")
df_contents.head()

"""### performing word embedding"""

def get_embeddings(text, model="text-embedding-ada-002"):
    text = text.replace("\n", " ")
    return client.embeddings.create(input = [text], model=model).data[0].embedding



get_embeddings(df_contents["Content "].iloc[0])[:5]

"""### Adding Embeddings on our File"""

df_contents['Embeddings'] =df_contents['Content '].apply(lambda x: get_embeddings(x))

df_contents.head()

question_embedding = get_embeddings(Question)
Question, question_embedding[:5]



"""### Using dot product to find the similarities between my qustio and the file info"""

def fn_dot_pro(embeddings_):
  return np.dot(embeddings_, question_embedding)

df_contents['distance'] = df_contents['Embeddings'].apply(fn_dot_pro)

df_contents.shape

df_contents.head(6)



"""### Sort to get the file with the top similarity between my questions and embeddings"""

df_contents.sort_values('distance', ascending=False, inplace=True)
df_contents.head()



"""### getting the top 3 docs from the table that has the highest similarity"""

context = df_contents.iloc[0]['Content '] + "\n" + df_contents.iloc[1]['Content '] + "\n" + df_contents.iloc[2]['Content ']+ "\n" + df_contents.iloc[3]['Content ']
print(context)
#

context

"""### using an LLM - gpt to answer the questions based on the informatiom with high similarity"""

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "system", "content":"Answer in oxford english"},
        {"role": "user", "content": Question},
        {"role":"assistant","content": f'use the information in {context} to answer my question'}
    ])

print(response.choices[0].message.content)

